{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import os\n",
    "import shutil\n",
    "import pyspark as ps\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "# create spark context\n",
    "spark = (ps.sql.SparkSession.builder\n",
    "        .appName(\"ALS model\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in model\n",
    "\n",
    "We will use user clusters as part of user features for our model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o29.load.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Ljava/lang/String;)Lorg/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat;\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1468)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1462)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1503)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1503)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:563)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m final_model \u001b[38;5;241m=\u001b[39m \u001b[43mALSModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./model_5/bestmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\ml\\util.py:353\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\ml\\util.py:302\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[1;32m--> 302\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[0;32m    306\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o29.load.\n: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Ljava/lang/String;)Lorg/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat;\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1468)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1462)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1503)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:407)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1503)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.recommendation.ALSModel$ALSModelReader.load(ALS.scala:563)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n"
     ]
    }
   ],
   "source": [
    "final_model = ALSModel.load(\"./model_5/bestmodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:======================================================>(99 + 1) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|customer_index|     recommendations|\n",
      "+--------------+--------------------+\n",
      "|            26|[{12745, 0.997935...|\n",
      "|            27|[{7645, 4.9884496...|\n",
      "|            28|[{9397, 0.946731}...|\n",
      "|            31|[{1693, 4.882163}...|\n",
      "+--------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# generate top_n product recommendations for user\n",
    "nrecommend = 5\n",
    "user_recs = final_model.recommendForAllUsers(nrecommend)\n",
    "user_recs.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "recs = user_recs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "user_features_df = pd.read_csv(\"./model_data/user_model.csv\")\n",
    "products = pd.read_csv(\"./model_data/item_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrecommend = 5\n",
    "def user_recommendations(user_id, top_n = 3):\n",
    "    \n",
    "    if top_n > nrecommend:\n",
    "        print(\"Please select up to {} items to recommend\".format(nrecommend))\n",
    "        return; \n",
    "    \n",
    "    prior_purchases = user_features_df[user_features_df['customer_unique_id'] == user_id]\\\n",
    "                                                                                        ['product_id'].unique()\n",
    "    num_items = len(prior_purchases)\n",
    "    \n",
    "    if num_items < 3:\n",
    "        items = num_items\n",
    "    else:\n",
    "        items = 3\n",
    "    \n",
    "    print(\"User: {}\\n\".format(user_id))\n",
    "    print(\"Known positives: \")\n",
    "    for n in range(items):\n",
    "        known_like_product = user_features_df[user_features_df['customer_unique_id'] == user_id]\\\n",
    "                                                            ['product_id'].unique()[n]\n",
    "        known_like_category = products[products['product_id'] == known_like_product]\\\n",
    "                                                            ['product_category_name'].unique()[0]\n",
    "    \n",
    "        print(\"\\t\", known_like_product)\n",
    "        print(\"\\t\", known_like_category, \"\\n\")\n",
    "    \n",
    "    \n",
    "    customer_index = user_features_df[user_features_df['customer_unique_id'] == user_id]\\\n",
    "                                                            ['customer_index'].unique()[0]\n",
    "    print(\"Top {} Recommendations: \\n\".format(top_n))\n",
    "    rec_products = []\n",
    "    \n",
    "    for n in range(top_n):\n",
    "        \n",
    "        rec_products.append(list(recs[recs['customer_index'] == customer_index]['recommendations'])[0][n][0])\n",
    "        \n",
    "        print(\"{}.\\n\".format(n+1), products[products['product_index'] == rec_products[n]]\\\n",
    "                                                  [['product_id', 'product_category_name']].iloc[0][0])\n",
    "        \n",
    "        print(products[products['product_index'] == rec_products[n]]\\\n",
    "                                                  [['product_id', 'product_category_name']].iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test for customer_id = 'c8ed31310fc440a3f8031b177f9842c3'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: c8ed31310fc440a3f8031b177f9842c3\n",
      "\n",
      "Known positives: \n",
      "\t 1065e0ebef073787a7bf691924c60eeb\n",
      "\t construction_tools_construction \n",
      "\n",
      "\t 0cf2faf9749f53924cea652a09d8e327\n",
      "\t construction_tools_construction \n",
      "\n",
      "\t 0de59eddc63167215c972b0d785ffa7b\n",
      "\t construction_tools_construction \n",
      "\n",
      "Top 5 Recommendations: \n",
      "\n",
      "1.\n",
      " 3cc4d754aa53f733c4141170236195ec\n",
      "computers_accessories\n",
      "2.\n",
      " 2c3dfe5b09e660349378f30d628d70bb\n",
      "construction_tools_construction\n",
      "3.\n",
      " f399bbedb1d21192dffefc4a8b30a229\n",
      "housewares\n",
      "4.\n",
      " d68eca92c5a12467b856479934dc39af\n",
      "telephony\n",
      "5.\n",
      " 6b770b5934dbdb29064efdf34bfa6cf5\n",
      "sports_leisure\n"
     ]
    }
   ],
   "source": [
    "user_recommendations('c8ed31310fc440a3f8031b177f9842c3', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test for customer_id = '698e1cf81d01a3d389d96145f7fa6df8'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: b56d31572e47b1e6d1b88d3128f2226b\n",
      "\n",
      "Known positives: \n",
      "\t 9d364fec2ac9a80f64bae58b5c034832\n",
      "\t housewares \n",
      "\n",
      "Top 5 Recommendations: \n",
      "\n",
      "1.\n",
      " 5dddb31154cbd968caa4706ef0f4e0f0\n",
      "garden_tools\n",
      "2.\n",
      " 86df1e9a07c3d463f6a6c3f776051e9d\n",
      "housewares\n",
      "3.\n",
      " 6a3370dbb5bd5817ac81a5db12cb2baa\n",
      "perfumery\n",
      "4.\n",
      " 12a5914d1826c08632a428b769a4eb30\n",
      "health_beauty\n",
      "5.\n",
      " 9c1e194db1d35a79d962ea610bfe0868\n",
      "perfumery\n"
     ]
    }
   ],
   "source": [
    "user_recommendations('b56d31572e47b1e6d1b88d3128f2226b', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test for customer_id = '89be58cbdd6ef318e3ed93fdb22be178'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 89be58cbdd6ef318e3ed93fdb22be178\n",
      "\n",
      "Known positives: \n",
      "\t 3fdb534dccf5bc9ab0406944b913787d\n",
      "\t diapers_and_hygiene \n",
      "\n",
      "Top 5 Recommendations: \n",
      "\n",
      "1.\n",
      " 0723afa6f9a5a11c512396db0bb03051\n",
      "stationery\n",
      "2.\n",
      " 0efaf4b155c6545bdc426ec0722cceaa\n",
      "toys\n",
      "3.\n",
      " 743c14ed5f01c78944d28320cb2cb8e2\n",
      "books_general_interest\n",
      "4.\n",
      " 551676a4458c024f741721013d87afc5\n",
      "auto\n",
      "5.\n",
      " fc8e852906ed3985a6e1ea67eb1dfdf9\n",
      "furniture_decor\n"
     ]
    }
   ],
   "source": [
    "user_recommendations('89be58cbdd6ef318e3ed93fdb22be178', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear after producing a simple recommendation system with matrix factorization using only users prior purchase history that this dataset simply does not have the data necessary to give accurate results. Many of the attempted recommendations produced results that are clearly not relevant for the user. As can be seen in the suggestions above, many of the top items are from categories very different from the original purchase. \n",
    "\n",
    "Comparing these same recommendations with LightFM shows how well hybrid recommenders can do for data sets like this one, with very few return users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
