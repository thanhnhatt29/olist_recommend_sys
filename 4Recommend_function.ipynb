{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import os\n",
    "import shutil\n",
    "import pyspark as ps\n",
    "from pyspark.ml import Pipeline, Transformer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS, ALSModel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.0\n"
     ]
    }
   ],
   "source": [
    "# create spark context\n",
    "spark = (ps.sql.SparkSession.builder\n",
    "        .appName(\"ALS model\")\n",
    "        .getOrCreate()\n",
    "        )\n",
    "sc = spark.sparkContext\n",
    "print(spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "We will use user clusters as part of user features for our model here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source data from prior step\n",
    "data_dir = os.path.join(\"clean_data\", \"\")\n",
    "file = os.path.join(data_dir, \"spark_master.csv\")\n",
    "\n",
    "# options are specified to read in data without error\n",
    "df_master = spark.read.format(\"csv\")\\\n",
    "               .option(\"multiline\", \"true\")\\\n",
    "               .option(\"header\", \"true\")\\\n",
    "               .option(\"inferSchema\", \"true\")\\\n",
    "               .load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = df_master.select(df_master['customer_unique_id'], \n",
    "                               df_master['product_id'], \n",
    "                               df_master['review_score'])\n",
    "user_features = user_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features = df_master.select(df_master['product_id'], \n",
    "                               df_master['product_category_name'])\n",
    "item_features = item_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+-------------+\n",
      "|  customer_unique_id|customer_index|          product_id|product_index|\n",
      "+--------------------+--------------+--------------------+-------------+\n",
      "|0000366f3b9a7992b...|        5166.0|372645c7439f9661f...|        451.0|\n",
      "|0000b849f77a49e4a...|        5167.0|5099f7000472b634f...|       2313.0|\n",
      "|0000f46a3911fa3c0...|        5168.0|64b488de448a5324c...|       3486.0|\n",
      "|0000f6ccb0745a6a4...|        5169.0|2345a354a6f203360...|       5592.0|\n",
      "|0004aac84e0df4da2...|        5170.0|c72e18b3fe2739b8d...|      27381.0|\n",
      "|0004bd2a26a76fe21...|        5171.0|25cf184645f3fae66...|       5607.0|\n",
      "|00050ab1314c0e55a...|        5172.0|8cefe1c6f2304e7e6...|       2900.0|\n",
      "|00053a61a98854899...|         726.0|62984ea1bba7fcea1...|       4526.0|\n",
      "|00053a61a98854899...|         726.0|58727e154e8e85d84...|       1134.0|\n",
      "|0005e1862207bf6cc...|        5173.0|e24f73b7631ee3fbb...|        887.0|\n",
      "+--------------------+--------------+--------------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# create object of StringIndexer class and specify input and output column\n",
    "SI_customer = StringIndexer(inputCol='customer_unique_id',outputCol='customer_index')\n",
    "SI_product = StringIndexer(inputCol='product_id',outputCol='product_index')\n",
    "\n",
    "# transform the data\n",
    "user_features = SI_customer.fit(user_features).transform(user_features)\n",
    "user_features = SI_product.fit(user_features).transform(user_features)\n",
    "\n",
    "# view the transformed data\n",
    "user_features.select('customer_unique_id', 'customer_index', 'product_id', 'product_index').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|customer_index|     recommendations|\n",
      "+--------------+--------------------+\n",
      "|            28|[{6490, 1.3932567...|\n",
      "|            31|[{2857, 4.959978}...|\n",
      "|            34|[{879, 4.951983},...|\n",
      "|            53|[{11275, 4.985459...|\n",
      "+--------------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate top_n product recommendations for user\n",
    "nrecommend = 5\n",
    "user_recs = final_model.recommendForAllUsers(nrecommend)\n",
    "user_recs.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = user_recs.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Generate pandas df for accessing products in recommender function\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m products \u001b[38;5;241m=\u001b[39m \u001b[43mitem_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\sql\\dataframe.py:1217\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m   1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcollectToPython()\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_load_from_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatchedSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCPickleSerializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\serializers.py:152\u001b[0m, in \u001b[0;36mFramedSerializer.load_stream\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 152\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_with_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\serializers.py:166\u001b[0m, in \u001b[0;36mFramedSerializer._read_with_length\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_with_length\u001b[39m(\u001b[38;5;28mself\u001b[39m, stream):\n\u001b[1;32m--> 166\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[43mread_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m SpecialLengths\u001b[38;5;241m.\u001b[39mEND_OF_DATA_SECTION:\n\u001b[0;32m    168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\site-packages\\pyspark\\serializers.py:594\u001b[0m, in \u001b[0;36mread_int\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_int\u001b[39m(stream):\n\u001b[1;32m--> 594\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m length:\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nhatt\\miniconda3\\envs\\sparkml\\lib\\socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Generate pandas df for accessing products in recommender function\n",
    "products = item_features.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features_df = user_features.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_recommendations(user_id, top_n = 3):\n",
    "    \n",
    "    if top_n > nrecommend:\n",
    "        print(\"Please select up to {} items to recommend\".format(nrecommend))\n",
    "        return; \n",
    "    \n",
    "    prior_purchases = user_features_df[user_features_df['customer_unique_id'] == user_id]\\\n",
    "                                                                                        ['product_id'].unique()\n",
    "    num_items = len(prior_purchases)\n",
    "    \n",
    "    if num_items < 3:\n",
    "        items = num_items\n",
    "    else:\n",
    "        items = 3\n",
    "    \n",
    "    print(\"User: {}\\n\".format(user_id))\n",
    "    print(\"Known positives: \")\n",
    "    for n in range(items):\n",
    "        known_like_product = user_features_df[user_features_df['customer_unique_id'] == user_id]\\\n",
    "                                                            ['product_id'].unique()[n]\n",
    "        known_like_category = products[products['product_id'] == known_like_product]\\\n",
    "                                                            ['product_category_name'].unique()[0]\n",
    "    \n",
    "        print(\"\\t\", known_like_product)\n",
    "        print(\"\\t\", known_like_category, \"\\n\")\n",
    "    \n",
    "    \n",
    "    customer_index = user_features_df[user_features_df['customer_unique_id'] == user_id]\\\n",
    "                                                            ['customer_index'].unique()[0]\n",
    "    print(\"Top {} Recommendations: \\n\".format(top_n))\n",
    "    rec_products = []\n",
    "    \n",
    "    for n in range(top_n):\n",
    "        \n",
    "        rec_products.append(list(recs[recs['customer_index'] == customer_index]['recommendations'])[0][n][0])\n",
    "        \n",
    "        print(\"{}.\\n\".format(n+1), products[products['product_index'] == rec_products[n]]\\\n",
    "                                                  [['product_id', 'product_category_name']].iloc[0][0])\n",
    "        \n",
    "        print(products[products['product_index'] == rec_products[n]]\\\n",
    "                                                  [['product_id', 'product_category_name']].iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test for customer_id = 'c8ed31310fc440a3f8031b177f9842c3'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: c8ed31310fc440a3f8031b177f9842c3\n",
      "\n",
      "Known positives: \n",
      "\t 1065e0ebef073787a7bf691924c60eeb\n",
      "\t construction_tools_construction \n",
      "\n",
      "\t 0cf2faf9749f53924cea652a09d8e327\n",
      "\t construction_tools_construction \n",
      "\n",
      "\t 309dd69eb83cea38c51709d62befe1a4\n",
      "\t construction_tools_construction \n",
      "\n",
      "Top 5 Recommendations: \n",
      "\n",
      "1.\n",
      " 15b1f9b06d0e709552d7d8638387e09b\n",
      "furniture_decor\n",
      "2.\n",
      " 7189fb70393a0b87189f93f19655f8db\n",
      "toys\n",
      "3.\n",
      " 3e7ec3672e5549ba74cf635752bfc70b\n",
      "furniture_decor\n",
      "4.\n",
      " 14ad6805c263d8d758d648f46a06570e\n",
      "baby\n",
      "5.\n",
      " 329c661807f085964b1877bfeca6ff73\n",
      "furniture_decor\n"
     ]
    }
   ],
   "source": [
    "user_recommendations('c8ed31310fc440a3f8031b177f9842c3', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test for customer_id = '698e1cf81d01a3d389d96145f7fa6df8'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 698e1cf81d01a3d389d96145f7fa6df8\n",
      "\n",
      "Known positives: \n",
      "\t 9571759451b1d780ee7c15012ea109d4\n",
      "\t auto \n",
      "\n",
      "Top 5 Recommendations: \n",
      "\n",
      "1.\n",
      " 0a4f9f421af66d2ea061fbb8883419f7\n",
      "health_beauty\n",
      "2.\n",
      " fdd84aefb08c8f8225e0b8c97429d53b\n",
      "health_beauty\n",
      "3.\n",
      " 12485f9cdebb6ca179826ede539554ad\n",
      "air_conditioning\n",
      "4.\n",
      " 616042729c11849827291496b18e9ec5\n",
      "sports_leisure\n",
      "5.\n",
      " 7a5c07212703b5f01ee199d29a29a587\n",
      "cool_stuff\n"
     ]
    }
   ],
   "source": [
    "user_recommendations('698e1cf81d01a3d389d96145f7fa6df8', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Test for customer_id = '89be58cbdd6ef318e3ed93fdb22be178'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 89be58cbdd6ef318e3ed93fdb22be178\n",
      "\n",
      "Known positives: \n",
      "\t 3fdb534dccf5bc9ab0406944b913787d\n",
      "\t diapers_and_hygiene \n",
      "\n",
      "Top 5 Recommendations: \n",
      "\n",
      "1.\n",
      " 779dd392d4fbe5ca656bf3ceabecbf0b\n",
      "construction_tools_construction\n",
      "2.\n",
      " bdcf6a834e8faa30dac3886c7a58e92e\n",
      "health_beauty\n",
      "3.\n",
      " 91b08d34d0ba4db44da2dc382867ba49\n",
      "telephony\n",
      "4.\n",
      " 1b8ee158f59c098470fad33f39660964\n",
      "furniture_living_room\n",
      "5.\n",
      " d9339c5714743c460a9470730f79f6c5\n",
      "computers_accessories\n"
     ]
    }
   ],
   "source": [
    "user_recommendations('89be58cbdd6ef318e3ed93fdb22be178', top_n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear after producing a simple recommendation system with matrix factorization using only users prior purchase history that this dataset simply does not have the data necessary to give accurate results. Many of the attempted recommendations produced results that are clearly not relevant for the user. As can be seen in the suggestions above, many of the top items are from categories very different from the original purchase. \n",
    "\n",
    "Comparing these same recommendations with LightFM shows how well hybrid recommenders can do for data sets like this one, with very few return users. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
